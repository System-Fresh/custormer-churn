# -*- coding: utf-8 -*-
"""22212860_Nnamdi_Ezema_customer-churn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16dgsqj5x3LE3SMxSnVcwLi-QAm-ntjGx
"""

#!pip install tensorflow

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import warnings
from imblearn.over_sampling import SMOTE

from sklearn import metrics
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, classification_report
from sklearn.linear_model import LogisticRegression
import xgboost as xgb
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten
from sklearn.metrics import accuracy_score
from tensorflow.keras import regularizers
from sklearn.model_selection import GridSearchCV

tf.random.set_seed(3)

from google.colab import files
upload = files.upload()

df = pd.read_csv('customer_churn_dataset-training-master.csv')

df

df.describe()

df.info()

df.loc[df['Gender'] == 'Female', 'Gender'] = 1
df.loc[df['Gender'] == 'Male', 'Gender'] = 0

df.loc[df['Contract Length'] == 'Monthly', 'Contract Length'] = 2
df.loc[df['Contract Length'] == 'Quarterly', 'Contract Length'] = 1
df.loc[df['Contract Length'] == 'Annual', 'Contract Length'] = 0

df.loc[df['Subscription Type'] == 'Premium', 'Subscription Type'] = 2
df.loc[df['Subscription Type'] == 'Standard', 'Subscription Type'] = 1
df.loc[df['Subscription Type'] == 'Basic', 'Subscription Type'] = 0

encoder = LabelEncoder()

columns_to_encode = ['Gender', 'Subscription Type', 'Contract Length']

for column in columns_to_encode:
    df[column] = encoder.fit_transform(df[column])

df

df.isna().sum()

df = df.dropna()
df.isna().sum()

df.head(3)

df['Gender'].value_counts()

sns.countplot(data=df, x='Churn', palette='rainbow')
print(df['Churn'].value_counts())

churn_counts = df['Churn'].value_counts()

# Plotting the pie chart
plt.figure(figsize=(6, 6))
plt.pie(churn_counts, labels=churn_counts.index, autopct='%1.1f%%', colors=['lightcoral', 'lightskyblue'])
plt.title('Percentage of Churn Status')
plt.show()

df = df.drop('CustomerID', axis=1)
df.head(5)

cat_cols = ['Gender', 'Subscription Type', 'Contract Length','Support Calls', 'Churn']
plt.figure(figsize=(15, 15))
for n, variable in enumerate(cat_cols):
    ax = plt.subplot(3, 2, n + 1)
    g=sns.countplot(data=df, x=df[variable], ax=ax, palette='coolwarm')
plt.show()

plt.tight_layout()

df.head(5)

df_X = df.drop(['Churn'], axis=1)
df_Y = df['Churn']

df_X.shape

df_Y.shape

df_Y

smote_ = SMOTE(random_state=0, k_neighbors=10)

dfX_col = df_X.columns

print(df_Y.value_counts())

os_data_X, os_data_y = smote_.fit_resample(df_X, df_Y)
os_data_X = pd.DataFrame(data=os_data_X, columns=dfX_col)
os_data_y = pd.DataFrame(data=os_data_y, columns=['Churn'])

print(os_data_X.shape)
print('---------------------')
print(os_data_y.value_counts())

dfX_col

df_X.head(5)

plt.figure(figsize=(5,3))
sns.countplot(data=os_data_y, x='Churn')
plt.show()

X_train = os_data_X
y_train = os_data_y

plt.subplots(figsize = (20, 20))
plt.title('Confusion Matrix to get the correlation ', fontsize=12)
sns.heatmap(df.corr(), annot=True)
plt.show()

X_train, X_test, Y_train, Y_test = train_test_split(X_train, y_train, test_size=0.3, random_state=0)

encoder = LabelEncoder()

columns_to_encode = ['Gender', 'Subscription Type', 'Contract Length']

for column in columns_to_encode:
    df[column] = encoder.fit_transform(df[column])

#Normalization of Features

from sklearn.preprocessing import StandardScaler
# Create an instance of the scaler
scaler = StandardScaler()

# Fit the scaler on the training data
scaler.fit(X_train)

# Transform the training data
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Function to print out training accuracy scores
def training_scores(y_act, y_pred):
    acc = round(accuracy_score(y_act, y_pred), 3)
    f1 = round(f1_score(y_act, y_pred), 3)
    print(f'Training Scores: Accuracy={acc}, F1-Score={f1}')

def validation_scores(y_act, y_pred):
    acc = round(accuracy_score(y_act, y_pred), 3)
    f1 = round(f1_score(y_act, y_pred), 3)
    print(f'Validation Scores: Accuracy={acc}, F1-Score={f1}')

# Instantiate the model
logReg = LogisticRegression(max_iter=1000)

#Fit the data into the model || Train the model
logReg.fit(X_train, Y_train)

# Predict on a unknown dataset(X_test)
y_pred = logReg.predict(X_test)
y_predt = logReg.predict(X_train)

#Generate Accuracy Report
training_scores(Y_train, y_predt)
validation_scores(Y_train, y_predt)

print("Accuracy: ", accuracy_score(Y_test, y_pred))# Predicting the results
print(metrics.classification_report(Y_test, y_pred))

ConfusionMatrix_logReg  = confusion_matrix(Y_test, y_pred)
print(ConfusionMatrix_logReg)

correct_predictions = np.sum(np.diag(ConfusionMatrix_logReg))
total_predictions = np.sum(ConfusionMatrix_logReg)

accuracy = correct_predictions / total_predictions
print(accuracy)

matrix_logReg = sns.heatmap(ConfusionMatrix_logReg,square=True, annot=True, cmap='Blues', fmt='d', cbar=False)
plt.title('Confusion Matrix for Logistic Regression prediction')
plt.show(matrix_logReg)
plt.show()

# Function to print out training accuracy scores
def training_scores(y_act, y_pred):
    acc = round(accuracy_score(y_act, y_pred), 3)
    f1 = round(f1_score(y_act, y_pred), 3)
    print(f'Training Scores: Accuracy={acc}, F1-Score={f1}')

def validation_scores(y_act, y_pred):
    acc = round(accuracy_score(y_act, y_pred), 3)
    f1 = round(f1_score(y_act, y_pred), 3)
    print(f'Validation Scores: Accuracy={acc}, F1-Score={f1}')

# Create an instance of the XGBClassifier
model = xgb.XGBClassifier(objective='binary:logistic', random_state=3)

# Train the model on the training data
model.fit(X_train, Y_train)

# Make predictions on the test data
y_pred_xgb = model.predict(X_test)
y_predt = model.predict(X_train)


#Generate Accuracy Report
training_scores(Y_train, y_predt)
validation_scores(Y_train, y_predt)

# Evaluate the model's performance
accuracy = accuracy_score(Y_test, y_pred_xgb)
print(f"Accuracy: {accuracy}")
print(metrics.classification_report(Y_test, y_pred_xgb))

ConfusionMatrix_xgb  = confusion_matrix(Y_test, y_pred_xgb)
print(ConfusionMatrix_xgb)

correct_predictions = np.sum(np.diag(ConfusionMatrix_xgb))
total_predictions = np.sum(ConfusionMatrix_xgb)

accuracy = correct_predictions / total_predictions
print(accuracy)

matrix_xgb = sns.heatmap(ConfusionMatrix_xgb,annot=True,cmap=plt.cm.Blues)
plt.title('Confusion Matrix for XGBoost prediction')
plt.show(matrix_xgb)
plt.show()

# Function to print out training accuracy scores
def training_scores(y_act, y_pred):
    acc = round(accuracy_score(y_act, y_pred), 3)
    f1 = round(f1_score(y_act, y_pred), 3)
    print(f'Training Scores: Accuracy={acc}, F1-Score={f1}')

def validation_scores(y_act, y_pred):
    acc = round(accuracy_score(y_act, y_pred), 3)
    f1 = round(f1_score(y_act, y_pred), 3)
    print(f'Validation Scores: Accuracy={acc}, F1-Score={f1}')

#Instantiate the Model
RF_Model = RandomForestClassifier(n_estimators=100, criterion='entropy',random_state=1)

#Train the model
RF_Model.fit(X_train,Y_train)

#Predict on an unknown dataset...
RF_Predict = RF_Model.predict(X_test)

y_predt = RF_Model.predict(X_train)


#Generate Accuracy Report
training_scores(Y_train, y_predt)
validation_scores(Y_train, y_predt)

# Evaluate the model's performance
RF_Model_accuracy = round(accuracy_score(Y_test,RF_Predict)*100,2)
print(f"Accuracy: {RF_Model_accuracy}")
print(metrics.classification_report(Y_test, RF_Predict))

ConfusionMatrix_RF = confusion_matrix(Y_test, RF_Predict)
print(ConfusionMatrix_RF)

correct_predictions = np.sum(np.diag(ConfusionMatrix_RF))
total_predictions = np.sum(ConfusionMatrix_RF)

accuracy = correct_predictions / total_predictions
print(accuracy)

matrix_RF = sns.heatmap(ConfusionMatrix_RF,annot=True,cmap=plt.cm.Blues)
plt.title('Confusion Matrix for Random Forest prediction')
plt.show(matrix_RF)
plt.show()

X_train_scaled.shape

model = Sequential([
    keras.layers.Flatten(input_shape=(10,)),
    keras.layers.Dense(20, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    keras.layers.Dense(40, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    keras.layers.Dense(2, activation='sigmoid')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

parameters = {'C': [0.1, 1, 10, 100], 'penalty': ['l1', 'l2']}
grid_search = GridSearchCV(LogisticRegression(max_iter=1000), parameters, cv=5)
grid_search.fit(X_train_scaled, Y_train)
print(grid_search.best_params_)

history_ = model.fit(X_train_scaled, Y_train, epochs=10, batch_size=32, validation_split=0.1)

model = Sequential([
    keras.layers.Flatten(input_shape=(10,)),
    keras.layers.Dense(20, activation='relu'),
    keras.layers.Dense(40, activation='relu'),
    keras.layers.Dense(2, activation='sigmoid')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

history_ = model.fit(X_train_scaled, Y_train, epochs=10, batch_size=32, validation_split=0.1)

plt.plot(history_.history['accuracy'])
plt.plot(history_.history['val_accuracy'])

plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['training data', 'validation data'], loc='lower right')

plt.plot(history_.history['loss'])
plt.plot(history_.history['val_loss'])

plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['training data', 'validation data'], loc='upper right')

#Accuracy of the model on test, firstly...
loss, accuracy = model.evaluate(X_test_scaled, Y_test)
print(f'Accuracy score: {round(accuracy, 3)}')

print(X_test_scaled.shape)
print(X_test_scaled[0])

#Prediction.
Y_pred_NN = model.predict(X_test_scaled) #the model.predict() returns probability not class label

print(Y_pred_NN.shape)
print(Y_pred_NN[0])
print("-"*40)
print(Y_pred_NN)

#the probability that it is 0 is 93% and the probaility that it is 1 is 4%. [0.9343195  0.04039179]

#Convert prediction probabilities back to label class

y_pred_labels = [np.argmax(i) for i in Y_pred_NN]
print(y_pred_labels)









ann_model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')
])

ann_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history = ann_model.fit(X_train_scaled, Y_train, epochs=10, batch_size=32, validation_data=(X_test_scaled, Y_test))

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])

plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['training data', 'validation data'], loc='lower right')

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])

plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['training data', 'validation data'], loc='upper right')

#Accuracy of the model on test, firstly...
loss, accuracy = ann_model.evaluate(X_test_scaled, Y_test)
print(f'Accuracy score: {round(accuracy, 3)}')

print()

ann_pred = (ann_model.predict(X_test_scaled) > 0.5).astype("int32")
ann_accuracy = accuracy_score(Y_test, ann_pred)
print("ANN Accuracy:", ann_accuracy)

ConfusionMatrix_ANN = confusion_matrix(Y_test, ann_pred)
print(ConfusionMatrix_ANN)

correct_predictions = np.sum(np.diag(ConfusionMatrix_ANN))
total_predictions = np.sum(ConfusionMatrix_ANN)

accuracy = correct_predictions / total_predictions
print(accuracy)

from sklearn.naive_bayes import GaussianNB

# Function to print out training accuracy scores
def training_scores(y_act, y_pred):
    acc = round(accuracy_score(y_act, y_pred), 3)
    f1 = round(f1_score(y_act, y_pred), 3)
    print(f'Training Scores: Accuracy={acc}, F1-Score={f1}')

def validation_scores(y_act, y_pred):
    acc = round(accuracy_score(y_act, y_pred), 3)
    f1 = round(f1_score(y_act, y_pred), 3)
    print(f'Validation Scores: Accuracy={acc}, F1-Score={f1}')

nb_model = GaussianNB()

nb_model.fit(X_train_scaled, Y_train)  # Using the scaled data for consistency

y_pred_nb = nb_model.predict(X_test_scaled)

y_predt = nb_model.predict(X_train_scaled)


#Generate Accuracy Report
training_scores(Y_train, y_predt)
validation_scores(Y_train, y_predt)

accuracy_nb = accuracy_score(Y_test, y_pred_nb)
print(f"Accuracy: {accuracy_nb}")
print(metrics.classification_report(Y_test, y_pred_nb))

confusion_matrix_nb = confusion_matrix(Y_test, y_pred_nb)
print(confusion_matrix_nb)

matrix_nb = sns.heatmap(confusion_matrix_nb, annot=True, cmap=plt.cm.Blues)
plt.title('Confusion Matrix for Naive Bayes prediction')
plt.show(matrix_nb)

from sklearn.tree import DecisionTreeClassifier

# Function to print out training accuracy scores
def training_scores(y_act, y_pred):
    acc = round(accuracy_score(y_act, y_pred), 3)
    f1 = round(f1_score(y_act, y_pred), 3)
    print(f'Training Scores: Accuracy={acc}, F1-Score={f1}')

def validation_scores(y_act, y_pred):
    acc = round(accuracy_score(y_act, y_pred), 3)
    f1 = round(f1_score(y_act, y_pred), 3)
    print(f'Validation Scores: Accuracy={acc}, F1-Score={f1}')

dt_model = DecisionTreeClassifier(random_state=0)

dt_model.fit(X_train_scaled, Y_train)

y_pred_dt = dt_model.predict(X_test_scaled)
y_predt = dt_model.predict(X_train_scaled)


#Generate Accuracy Report
training_scores(Y_train, y_predt)
validation_scores(Y_train, y_predt)

accuracy_dt = accuracy_score(Y_test, y_pred_dt)
print(f"Accuracy: {accuracy_dt}")
print(metrics.classification_report(Y_test, y_pred_dt))

# Print confusion matrix
confusion_matrix_dt = confusion_matrix(Y_test, y_pred_dt)
print(confusion_matrix_dt)

# Optional: Visualize confusion matrix
matrix_dt = sns.heatmap(confusion_matrix_dt, annot=True, cmap=plt.cm.Blues)
plt.title('Confusion Matrix for Decision Tree prediction')
plt.show(matrix_dt)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

# Define function to calculate precision, recall, f1-score, and accuracy for a model
def calculate_metrics(y_true, y_pred):
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    accuracy = accuracy_score(y_true, y_pred)
    return precision, recall, f1, accuracy

# Define models with their abbreviated names
models = {
    "Logistic Regression": "LR",
    "XGBoost": "XGB",
    "Random Forest": "RF",
    "Neural Network": "NN",
    "Naive Bayes": "NB",
    "Decision Tree": "DT"
}

# Initialize lists to store metrics
precision_scores = []
recall_scores = []
f1_scores = []
accuracy_scores = []

# Calculate metrics for each model
for model_name, model_abbr in models.items():
    if model_name == "Logistic Regression":
        precision, recall, f1, accuracy = calculate_metrics(Y_test, y_pred)
    elif model_name == "XGBoost":
        precision, recall, f1, accuracy = calculate_metrics(Y_test, y_pred_xgb)
    elif model_name == "Random Forest":
        precision, recall, f1, accuracy = calculate_metrics(Y_test, RF_Predict)
    elif model_name == "Neural Network":
        precision, recall, f1, accuracy = calculate_metrics(Y_test, ann_pred)
    elif model_name == "Naive Bayes":
        precision, recall, f1, accuracy = calculate_metrics(Y_test, y_pred_nb)
    elif model_name == "Decision Tree":
        precision, recall, f1, accuracy = calculate_metrics(Y_test, y_pred_dt)

    # Append metrics to respective lists
    precision_scores.append(precision)
    recall_scores.append(recall)
    f1_scores.append(f1)
    accuracy_scores.append(accuracy)

# Plotting the metrics
plt.figure(figsize=(10, 6))
plt.plot(models.values(), precision_scores, label='Precision', marker='o')
plt.plot(models.values(), recall_scores, label='Recall', marker='s')
plt.plot(models.values(), f1_scores, label='F1-Score', marker='^')
plt.plot(models.values(), accuracy_scores, label='Accuracy', marker='x')
plt.title('Model Performance Metrics')
plt.xlabel('Models')
plt.ylabel('Score')
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

import pandas as pd

# Create a DataFrame to store the metrics
metrics_df = pd.DataFrame({
    'Model': list(models.values()),
    'Precision': precision_scores,
    'Recall': recall_scores,
    'F1-Score': f1_scores,
    'Accuracy': accuracy_scores
})

# Display the DataFrame
print(metrics_df)
# Plotting the metrics
plt.figure(figsize=(12, 8))

# Plot precision
plt.bar(models.values(), precision_scores, label='Precision', width=0.15)

# Plot recall
plt.bar(models.values(), recall_scores, label='Recall', width=0.15, bottom=precision_scores)

# Plot f1-score
plt.bar(models.values(), f1_scores, label='F1-Score', width=0.15, bottom=[i+j for i,j in zip(precision_scores, recall_scores)])

# Plot accuracy
plt.bar(models.values(), accuracy_scores, label='Accuracy', width=0.15, bottom=[i+j+k for i,j,k in zip(precision_scores, recall_scores, f1_scores)])

plt.title('Model Performance Metrics')
plt.xlabel('Models')
plt.ylabel('Score')
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()